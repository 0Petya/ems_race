---
title: "Processing data"
author: "Peter Tran"
output: html_document
---

```{r}
library(lubridate)
library(tidyverse)
```

Before reading in any data, we want to run `./src/data/fuzzy_filter_dates.sh`, which does a fuzzy filter to cut down the size of the raw datasets.

# Processing EMS data

```{r}
date_time_format <- "%m/%d/%y %I:%M:%S %p"

ems <- read_csv("../../data/interim/EMS_Incident_Dispatch_Data_fuzzy_2019.csv") %>%
  mutate(INCIDENT_DATETIME = parse_date_time(INCIDENT_DATETIME, date_time_format),
         FIRST_ASSIGNMENT_DATETIME = parse_date_time(FIRST_ASSIGNMENT_DATETIME, date_time_format),
         FIRST_ACTIVATION_DATETIME = parse_date_time(FIRST_ACTIVATION_DATETIME, date_time_format),
         FIRST_ON_SCENE_DATETIME = parse_date_time(FIRST_ON_SCENE_DATETIME, date_time_format),
         FIRST_TO_HOSP_DATETIME = parse_date_time(FIRST_TO_HOSP_DATETIME, date_time_format),
         FIRST_HOSP_ARRIVAL_DATETIME = parse_date_time(FIRST_HOSP_ARRIVAL_DATETIME, date_time_format),
         INCIDENT_CLOSE_DATETIME = parse_date_time(INCIDENT_CLOSE_DATETIME, date_time_format)) %>%
  filter(INCIDENT_DATETIME >= as.Date("2019-01-01") & INCIDENT_DATETIME < as.Date("2020-01-01"))
```

If for any reason the response time is not populated, we can't use that data.

```{r}
ems <- filter(ems, !is.na(INCIDENT_RESPONSE_SECONDS_QY))
```

```{r}
summary(ems$INCIDENT_RESPONSE_SECONDS_QY)
```

That's quite the range in response time, some people are waiting for several hours. Additionaly, we don't want any response times of 0 seconds, that makes no sense.

```{r}
ems <- filter(ems, INCIDENT_RESPONSE_SECONDS_QY > 0)
```

```{r}
colnames(ems)
```

```{r}
unique(ems$INITIAL_CALL_TYPE)
```

```{r}
unique(ems$INITIAL_SEVERITY_LEVEL_CODE)
```

```{r}
unique(ems$HELD_INDICATOR)
```

```{r}
group_by(ems, REOPEN_INDICATOR) %>%
  summarize(n())
```

```{r}
unique(ems$SPECIAL_EVENT_INDICATOR)
```

```{r}
unique(ems$STANDBY_INDICATOR)
```

```{r}
unique(ems$TRANSFER_INDICATOR)
```

# Processing Census data

```{r}
census <- read_csv("../../data/raw/ACSDT5Y2019.B03002_data_with_overlays_2021-12-02T155548.csv", skip = 1) %>%
  setNames(c("total", "error", "total_not_hispanic_latino", "error_not_hispanic_latino", "total_not_hispanic_latino_white", "error_not_hispanic_latino_white", "total_not_hispanic_latino_black", "error_not_hispanic_latino_black", "total_not_hispanic_latino_american_indian_alaska_native", "error_not_hispanic_latino_american_indian_alaska_native", "total_not_hispanic_latino_asian", "error_not_hispanic_latino_asian", "total_not_hispanic_latino_native_hawaiian_pacific_islander", "error_not_hispanic_latino_native_hawaiian_pacific_islander", "total_not_hispanic_latino_other", "error_not_hispanic_latino_other", "total_not_hispanic_latino_multi_racial", "error_not_hispanic_latino_multi_racial", "total_not_hispanic_latino_multi_racial_two_races_including_other", "error_not_hispanic_latino_multi_racial_two_races_including_other", "total_not_hispanic_latino_multi_racial_two_races_excluding_other_and_three_or_more_races", "error_not_hispanic_latino_multi_racial_two_races_excluding_other_and_three_or_more_races", "total_hispanic_latino", "error_hispanic_latino", "total_hispanic_latino_white", "error_hispanic_latino_white", "total_hispanic_latino_black", "error_hispanic_latino_black", "total_hispanic_latino_american_indian_alaska_native", "error_hispanic_latino_american_indian_alaska_native", "total_hispanic_latino_asian", "error_hispanic_latino_asian", "total_hispanic_latino_native_hawaiian_pacific_islander", "error_hispanic_latino_native_hawaiian_pacific_islander", "total_hispanic_latino_other", "error_hispanic_latino_other", "total_hispanic_latino_multi_racial", "error_hispanic_latino_multi_racial", "total_hispanic_latino_multi_racial_two_races_including_other", "error_hispanic_latino_multi_racial_two_races_including_other", "total_hispanic_latino_multi_racial_two_races_excluding_other_and_three_or_more_races", "error_hispanic_latino_multi_racial_two_races_excluding_other_and_three_or_more_races", "id", "zip")) %>%
  mutate(zip = as.numeric(str_sub(zip, 7)))
```

We don't want raw numbers for race and ethnicity distributions, we want proportions.

```{r}
census_ratios <- data.frame(zip = census$zip, white = census$total_not_hispanic_latino_white / census$total,
                            black = census$total_not_hispanic_latino_black / census$total,
                            american_indian_alaska_native = census$total_not_hispanic_latino_american_indian_alaska_native / census$total,
                            asian = census$total_not_hispanic_latino_asian / census$total,
                            native_hawaiian_pacific_islander = census$total_not_hispanic_latino_native_hawaiian_pacific_islander / census$total,
                            other = census$total_not_hispanic_latino_other / census$total,
                            multi_racial = census$total_not_hispanic_latino_multi_racial / census$total,
                            hispanic_latino = census$total_hispanic_latino / census$total)
```

```{r}
ems <- inner_join(ems, census_ratios, by = c("ZIPCODE" = "zip")) %>%
  mutate(majority_white = white >= 0.5)
```

# Processing zip code data

```{r}
library(tigris)

options(tigris_use_cache = T)
```

We only need zip codes that exist in our EMS dataset.

```{r}
zips <- zctas(cb = T) %>%
  mutate(ZCTA5CE10 = as.numeric(ZCTA5CE10)) %>%
  filter(ZCTA5CE10 %in% ems$ZIPCODE)
```

# Processing traffic data

```{r}
library(parallel)
library(sf)
```

The coordinate system was determined from [here](https://datahub.cusp.nyu.edu/sites/default/files/documents/misc/NYC_Data_Geocoding_System.pdf).

```{r}
traffic <- read_csv("../../data/interim/Automated_Traffic_Volume_Counts_fuzzy_2019.csv") %>%
  filter(Yr == 2019) %>%
  mutate(datetime = parse_date_time(paste0(Yr, "-", M, "-", D, " ", HH, ":", MM), "ymd HM")) %>%
  st_as_sf(wkt = "WktGeom")

st_crs(traffic) <- 2263
traffic <- st_transform(traffic, st_crs(zips))
```

```{r}
system.time(traffic <- st_drop_geometry(st_join(zips, traffic, left = F)))
```

This takes a very long time to run! It is parallelized, so it may differ on your machine. With 4 cores, it took about 10 hours for me. Unfortunately it cannot be vectorized as that would take up too much memory.

```{r}
get_volume <- function(zip, incident_datetime, first_on_scene_datetime) {
  potential_recorders <- filter(traffic, ZCTA5CE10 == zip)
  recorders <- filter(potential_recorders, incident_datetime <= datetime & first_on_scene_datetime >= datetime)
  if (nrow(recorders) == 0) {
    recorders <- filter(potential_recorders, abs(first_on_scene_datetime - datetime) < min(15))
  }
  
  volume <- recorders %>%
    summarize(mean(Vol)) %>%
    pull
  
  return(volume)
}

system.time(ems$traffic <- mapply(get_volume, ems$ZIPCODE, ems$INCIDENT_DATETIME, ems$FIRST_ON_SCENE_DATETIME))
```

Some EMS calls were either placed in areas that had no recorders (rare) or no recorders were in operation during that time, since we may want to control for the traffic, that we have to remove those.

```{r}
ems_traffic <- filter(ems, !is.nan(traffic))
```

That's a whole order of magnitude smaller. We'll keep the full EMS dataset and model without traffic counts too.

Ok, now we can use that data for modelling.

```{r}
dir.create("../../data/processed/", showWarnings = F)
write_rds(ems, "../../data/processed/ems.rds")
write_csv(ems, "../../data/processed/ems.csv")
write_rds(ems_traffic, "../../data/processed/ems_traffic.rds")
write_csv(ems_traffic, "../../data/processed/ems_traffic.csv")
```
